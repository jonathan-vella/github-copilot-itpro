# Learning Service Validation with GitHub Copilot

A complete conversation transcript demonstrating how to learn Azure service validation
patterns through discovery-based dialogue with GitHub Copilot.

---

## The Scenario

**Character:** Marcus Chen  
**Role:** QA Engineer at HealthTech Systems  
**Background:** 6 years manual testing experience (functional testing, test case design, bug tracking)  
**Challenge:** Manual validation takes 25+ hours per deployment, often delaying releases  
**Goal:** Learn AI-assisted testing patterns while understanding WHY certain approaches work

**Project Context:**
- SAIF (Secure AI Framework) API deployed to Azure
- Python FastAPI application with SQL Database backend
- Managed identity authentication (no secrets)
- HIPAA compliance requirements
- Need: UAT validation + Load testing before production release

---

## Phase 1: Understanding Service Validation Patterns

### Starting the Conversation

**Marcus:** I'm a QA engineer who's been doing manual testing for years. I've heard about
using Copilot agents for testing, but I want to understand the patterns first. What makes
AI-assisted testing different from just automating test scripts?

**Copilot:** Great question! The key difference is the **workflow**, not just automation.

**Traditional Automation:**
```
Write test scripts â†’ Run scripts â†’ Get pass/fail â†’ Debug failures manually
```

**AI-Assisted Testing:**
```
Describe what to test â†’ Agent generates test plan â†’ Guided execution â†’ 
Intelligent analysis â†’ Actionable recommendations â†’ Auto-generated reports
```

**The Key Differences:**

| Aspect | Traditional Automation | AI-Assisted |
|--------|----------------------|-------------|
| Test planning | Manual, based on experience | Agent asks questions, generates comprehensive plan |
| Test coverage | What you remember to test | Agent suggests based on patterns |
| Result analysis | Parse logs manually | Agent identifies root causes |
| Documentation | Write reports manually | Auto-populated templates |
| Learning curve | High (scripting skills) | Low (conversational) |

The agent doesn't just run testsâ€”it guides you through a **structured validation workflow**
that ensures nothing is missed.

---

**Marcus:** That sounds helpful, but how does the agent know what to test? I have domain
expertise about our application that Copilot doesn't have.

**Copilot:** Exactly right! The agent combines YOUR domain knowledge with testing patterns.
Here's how it works:

**The Discovery Phase:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT ASKS                        â”‚ YOU PROVIDE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ What service are you testing?     â”‚ URL, API type, purpose      â”‚
â”‚ What endpoints are critical?      â”‚ Business-critical paths     â”‚
â”‚ What are acceptance criteria?     â”‚ Response times, error rates â”‚
â”‚ What security requirements?       â”‚ HIPAA, authentication       â”‚
â”‚ What's the expected load?         â”‚ Users, RPS, patterns        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What the Agent Adds:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TESTING PATTERNS                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ HTTP status code validation (200, 4xx, 5xx handling)          â”‚
â”‚ â€¢ Response time thresholds (p50, p95, p99)                      â”‚
â”‚ â€¢ Security headers (HTTPS, CORS, content-type)                  â”‚
â”‚ â€¢ Error response format (consistent error handling)             â”‚
â”‚ â€¢ Idempotency (safe to retry)                                   â”‚
â”‚ â€¢ Rate limiting (429 handling)                                  â”‚
â”‚ â€¢ Health check patterns (readiness, liveness)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Your expertise + Testing patterns = Comprehensive validation**

The agent isn't replacing your knowledgeâ€”it's ensuring you don't forget common testing
patterns while focusing on your domain-specific requirements.

---

### Understanding UAT vs. Load Testing

**Marcus:** I understand UATâ€”we do that for every release. But how is load testing different,
and when do I need both?

**Copilot:** Great question! They answer different questions about your service:

**UAT (User Acceptance Testing):**
```
Question: "Does the service work correctly?"

Focus:
- Functional correctness (endpoints return expected data)
- Business logic validation (calculations, workflows)
- Security compliance (authentication, authorization)
- Error handling (graceful failures, clear messages)
- Integration points (database, external services)

When: Before any deployment, after code changes
```

**Load Testing:**
```
Question: "Does the service work correctly UNDER PRESSURE?"

Focus:
- Throughput (requests per second)
- Response time degradation (p95 under load)
- Resource utilization (CPU, memory, connections)
- Breaking points (when does it fail?)
- Recovery behavior (does it come back?)

When: Before production, after scaling changes, capacity planning
```

**The Testing Pyramid for APIs:**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Load Tests    â”‚  â† "Can it handle the traffic?"
                    â”‚   (few, slow)   â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚  Integration    â”‚  â† "Does it work with dependencies?"
                    â”‚    Tests        â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚   UAT Tests     â”‚  â† "Does it meet acceptance criteria?"
                    â”‚  (many, fast)   â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚   Unit Tests    â”‚  â† "Does the code work?"
                    â”‚ (most, fastest) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**For your SAIF API deployment:**
1. Run UAT first (functional validation)
2. If UAT passes, run load tests (performance validation)
3. Both must pass before production release

---

**Marcus:** What makes good acceptance criteria? I usually just test "it works" but I know
that's not enough.

**Copilot:** Excellent self-awareness! Good acceptance criteria follow the **SMART** pattern
adapted for testing:

**SMART Acceptance Criteria:**

| Principle | Bad Example | Good Example |
|-----------|-------------|--------------|
| **S**pecific | "Fast response" | "Response time < 500ms" |
| **M**easurable | "Works well" | "200 status code, valid JSON" |
| **A**chievable | "Never fails" | "Error rate < 0.1%" |
| **R**elevant | "Pretty UI" (for API) | "Returns patient data correctly" |
| **T**ime-bound | "Eventually works" | "Within 5 seconds timeout" |

**Acceptance Criteria Categories:**

```
1. FUNCTIONAL
   âœ“ Endpoint returns expected data structure
   âœ“ Business logic produces correct results
   âœ“ Error cases return appropriate status codes
   âœ“ Edge cases are handled gracefully

2. PERFORMANCE
   âœ“ Response time < X ms (define threshold)
   âœ“ Throughput > X RPS under expected load
   âœ“ No memory leaks over extended operation

3. SECURITY
   âœ“ Authentication required for protected endpoints
   âœ“ No sensitive data in error messages
   âœ“ HTTPS enforced (no HTTP)
   âœ“ Proper headers (CORS, content-type)

4. RELIABILITY
   âœ“ Graceful degradation under load
   âœ“ Recovery after transient failures
   âœ“ Consistent behavior across restarts

5. COMPLIANCE (HIPAA for healthcare)
   âœ“ Audit logging enabled
   âœ“ Data encryption at rest and transit
   âœ“ Access controls enforced
```

**For your SAIF API, good criteria would be:**

```markdown
## Acceptance Criteria - SAIF API v2

### Functional
- [ ] Health endpoint returns 200 with service info
- [ ] SQL connectivity returns current user identity
- [ ] All endpoints return valid JSON

### Performance  
- [ ] Response time < 600ms for all endpoints
- [ ] Service handles 100 concurrent requests

### Security
- [ ] Managed identity authentication (no secrets)
- [ ] HTTPS enforced (HTTP returns redirect/error)
- [ ] No credentials in response bodies

### Compliance
- [ ] Database queries use managed identity (verify with /api/sqlwhoami)
- [ ] Connection from trusted Azure network (verify with /api/sqlsrcip)
```

---

## Phase 2: Using the UAT Assistant Agent

### Understanding Agent Workflow

**Marcus:** Okay, I understand the concepts. How does the UAT Assistant agent actually work?
Walk me through what happens when I use it.

**Copilot:** The UAT Assistant follows a structured 5-phase workflow:

**UAT Assistant Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: DISCOVERY                                                      â”‚
â”‚ Agent asks: Service URL, endpoints, acceptance criteria, security needs â”‚
â”‚ You provide: Domain knowledge, business requirements                    â”‚
â”‚ Output: Shared understanding of what to test                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PHASE 2: TEST PLANNING                                                  â”‚
â”‚ Agent generates: Comprehensive test checklist                           â”‚
â”‚ You review: Confirm coverage, add domain-specific tests                 â”‚
â”‚ Output: Agreed test plan (checklist format)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PHASE 3: TEST EXECUTION                                                 â”‚
â”‚ Agent guides: Run tests one-by-one or batch                             â”‚
â”‚ You execute: Run scripts or manual tests                                â”‚
â”‚ Output: Test results (pass/fail with details)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PHASE 4: RESULTS ANALYSIS                                               â”‚
â”‚ Agent analyzes: Parse results, identify patterns                        â”‚
â”‚ You clarify: Explain expected vs. actual for failures                   â”‚
â”‚ Output: Issue list with severity and recommendations                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PHASE 5: DOCUMENTATION                                                  â”‚
â”‚ Agent generates: Populate report template automatically                 â”‚
â”‚ You review: Add context, get sign-offs                                  â”‚
â”‚ Output: Audit-ready test report                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Let me show you a real conversation:**

---

### Live UAT Session Example

**Marcus:** @uat-assistant I need to run UAT tests for my SAIF API

**UAT Assistant:** I'll help you validate your SAIF API! Let me gather some information.

**What's the service URL?**

**Marcus:** https://app-saifv2-api-ss4xs2.azurewebsites.net

**UAT Assistant:** Got it. 

**What are the critical endpoints I should test?**

**Marcus:** The main ones are:
- `/` - Health check
- `/api/ip` - Client IP info  
- `/api/sqlwhoami` - Database identity
- `/api/sqlsrcip` - Database source IP

**UAT Assistant:** Perfect. 

**What are your acceptance criteria for this deployment?**

**Marcus:** All endpoints must:
- Return HTTP 200
- Respond in under 600ms
- Return valid JSON
- For SQL endpoints: show managed identity authentication (no password)

**UAT Assistant:** Great criteria! 

**Any security requirements I should validate?**

**Marcus:** Yes - this is HIPAA-sensitive. Need to verify:
- HTTPS only (no HTTP)
- No credentials exposed in responses
- Managed identity for database (no connection strings with passwords)

**UAT Assistant:** Excellent. Based on your requirements, here's the test plan:

```markdown
# UAT Test Plan - SAIF API v2

## 1. Health & Availability Tests
- [ ] Root endpoint (/) returns 200
- [ ] Response time < 600ms
- [ ] Response is valid JSON
- [ ] Contains service name and version

## 2. Client Information Tests  
- [ ] /api/ip returns 200
- [ ] Response time < 600ms
- [ ] Returns client IP address
- [ ] No sensitive data exposed

## 3. Database Connectivity Tests
- [ ] /api/sqlwhoami returns 200
- [ ] Shows managed identity user (not SQL auth)
- [ ] No password in response
- [ ] Response time < 600ms

- [ ] /api/sqlsrcip returns 200
- [ ] Shows Azure-internal IP (10.x or private)
- [ ] Response time < 600ms

## 4. Security Tests
- [ ] HTTP request redirects or fails (HTTPS only)
- [ ] No credentials in any response body
- [ ] Proper content-type headers
- [ ] No server version disclosure

## 5. Error Handling Tests
- [ ] Invalid endpoint returns 404
- [ ] 404 response is valid JSON
- [ ] Error message is user-friendly

**Total: 17 test cases across 5 categories**
```

Ready to execute? I can guide you through running tests one-by-one, or you can run the
automated test script.

**Marcus:** Let's run the automated script. Where is it?

**UAT Assistant:** The test script is in `validation/uat/`. Here's how to run it:

```bash
cd validation/uat
export API_BASE_URL="https://app-saifv2-api-ss4xs2.azurewebsites.net"
./uat-tests.sh
```

Run this and share the output with me. I'll analyze the results.

**Marcus:** [runs the script]

```
=== SAIF API v2 UAT Test Suite ===
API Base URL: https://app-saifv2-api-ss4xs2.azurewebsites.net

[TEST 1/24] Health Check - Root Endpoint
  âœ“ Status: 200 (expected: 200)
  âœ“ Response Time: 245ms (threshold: 600ms)
  âœ“ Valid JSON: true

[TEST 2/24] Health Check - Response Content
  âœ“ Contains service name
  âœ“ Contains version info

[TEST 3/24] Client IP Endpoint
  âœ“ Status: 200
  âœ“ Response Time: 189ms
  âœ“ Returns IP address

[TEST 4/24] SQL Who Am I
  âœ“ Status: 200
  âœ“ Response Time: 312ms
  âœ“ Shows managed identity user
  âœ“ No password in response

[TEST 5/24] SQL Source IP
  âœ“ Status: 200
  âœ“ Response Time: 287ms
  âœ“ Shows internal Azure IP

... [tests 6-24 continue] ...

=== TEST SUMMARY ===
Total Tests: 24
Passed: 24
Failed: 0
Pass Rate: 100%

Average Response Time: 256ms
Slowest Endpoint: /api/sqlwhoami (312ms)
Fastest Endpoint: /api/ip (189ms)
```

**UAT Assistant:** Excellent results! ğŸ‰

**Analysis:**

| Category | Tests | Passed | Status |
|----------|-------|--------|--------|
| Health & Availability | 5 | 5 | âœ… |
| Client Information | 4 | 4 | âœ… |
| Database Connectivity | 6 | 6 | âœ… |
| Security | 5 | 5 | âœ… |
| Error Handling | 4 | 4 | âœ… |
| **Total** | **24** | **24** | **100%** |

**Key Findings:**
- âœ… All endpoints respond correctly
- âœ… All response times under 600ms threshold (avg: 256ms)
- âœ… Managed identity confirmed (no SQL passwords)
- âœ… HTTPS enforcement working
- âœ… Error handling returns proper 404 with JSON

**Recommendations:**
- None - all acceptance criteria met!
- Consider adding performance baseline for future comparisons

Should I populate the UAT report template with these results?

**Marcus:** Yes please!

**UAT Assistant:** Report generated! You can find it at:
`validation/uat/uat-report-[timestamp].md`

The report includes:
- Executive summary (for stakeholders)
- Detailed test results by category
- Sign-off section (QA â†’ Tech Lead â†’ Business)

---

## Phase 3: Understanding Load Testing

### Why Load Testing Matters

**Marcus:** The UAT passed, so we're good to deploy, right? Why do we need load testing?

**Copilot:** Great question! UAT validates **correctness**, but production traffic creates
different pressures. Here's why load testing matters:

**The Iceberg Problem:**

```
                    What UAT Tests
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Correct?   â”‚  â† Single requests work
                    â”‚   âœ… Yes    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Production Reality
    
                    What Load Tests Reveal
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Connection pool exhaustion     â”‚
                    â”‚  Memory leaks under load        â”‚
                    â”‚  Database deadlocks             â”‚
                    â”‚  Thread starvation              â”‚
                    â”‚  Cold start penalties           â”‚
                    â”‚  Auto-scale timing gaps         â”‚
                    â”‚  Network bottlenecks            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Real-World Example:**
```
UAT Result: /api/sqlwhoami responds in 312ms âœ…
Load Test Result: At 200 concurrent users, response time degrades to 2.5s âŒ

Why? Database connection pool only has 10 connections.
At low load: Fine (connections available)
At high load: Requests queue waiting for connections
```

**Load testing reveals issues that only appear under pressure.**

---

**Marcus:** What metrics should I care about in load testing? I've seen graphs with lots of
lines and don't know what matters.

**Copilot:** Focus on these **key performance indicators (KPIs)**:

**Primary Metrics (Must Track):**

| Metric | What It Measures | Target (Typical) |
|--------|------------------|------------------|
| **RPS** (Requests/sec) | Throughput capacity | Depends on your traffic |
| **p95 Response Time** | 95th percentile latency | < 500ms for APIs |
| **Error Rate** | Failed requests percentage | < 1% (ideally < 0.1%) |
| **Availability** | Successful request ratio | > 99.9% |

**Why p95 Instead of Average?**

```
Scenario: 100 requests
- 95 requests: 200ms
- 5 requests: 5000ms (timeout/slow)

Average: 440ms â† Looks acceptable!
p95: 5000ms â† Shows real user pain!

p95 = "What do 95% of users experience?"
The worst 5% often includes your most important customers.
```

**Secondary Metrics (For Root Cause Analysis):**

| Metric | What It Reveals |
|--------|-----------------|
| **CPU %** | Compute bottleneck |
| **Memory %** | Memory leaks, GC pressure |
| **DB DTU/vCore** | Database bottleneck |
| **Connection Pool** | Connection exhaustion |
| **Network I/O** | Bandwidth limits |

**Performance Grading:**

```
Grade A: p95 < 200ms, error rate < 0.1%, hits RPS target
Grade B: p95 < 500ms, error rate < 0.5%, within 10% of target
Grade C: p95 < 1000ms, error rate < 1%, within 25% of target
Grade D: p95 < 2000ms, error rate < 2%, below target
Grade F: p95 > 2000ms or error rate > 2% or major failures
```

---

### Load Patterns Explained

**Marcus:** The agent asked about "load patterns" - what's the difference between ramp-up,
spike, sustained, and step?

**Copilot:** Each pattern answers different questions about your service:

**1. Ramp-Up Pattern**
```
Users
  ^
200â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚        â•±
100â”‚     â•±
   â”‚  â•±
 50â”‚â•±
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Time
        1min   2min   3min   4min

Question: "At what point does performance degrade?"
Use when: Capacity planning, finding limits
```

**2. Spike Pattern**
```
Users
  ^
200â”‚    â”Œâ”€â”€â”€â”€â”
   â”‚    â”‚    â”‚
100â”‚    â”‚    â”‚
   â”‚    â”‚    â”‚
 50â”‚â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Time

Question: "Can the system handle sudden traffic bursts?"
Use when: Testing auto-scale, flash sales, marketing campaigns
```

**3. Sustained Pattern**
```
Users
  ^
100â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚    â”‚
 50â”‚    â”‚
   â”‚    â”‚
   â”‚â”€â”€â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Time

Question: "Does performance degrade over time?"
Use when: Memory leak detection, stability testing
```

**4. Step Pattern**
```
Users
  ^
200â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚              â”Œâ”€â”€â”€â”€â”€â”˜
150â”‚        â”Œâ”€â”€â”€â”€â”€â”˜
   â”‚  â”Œâ”€â”€â”€â”€â”€â”˜
100â”‚â”€â”€â”˜
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Time

Question: "What's the breaking point?"
Use when: Finding maximum capacity, stress testing
```

**For your SAIF API deployment, I'd recommend:**
1. **Start with Ramp-Up** - Find your performance ceiling
2. **Then Spike** - Verify auto-scale behavior
3. **Finally Sustained** - Confirm stability

---

## Phase 4: Using the Load Test Assistant Agent

### Live Load Test Session

**Marcus:** @loadtest-assistant I need to load test my SAIF API

**Load Test Assistant:** I'll help you performance test your SAIF API. Let me gather
requirements.

**What's the service URL?**

**Marcus:** https://app-saifv2-api-ss4xs2.azurewebsites.net

**Load Test Assistant:** 

**Which endpoints should I test?**

**Marcus:** The main ones:
- `/` - Health check (lightweight)
- `/api/sqlwhoami` - Database query (heavier)
- `/api/sqlsrcip` - Database query (heavier)

**Load Test Assistant:** Good mix of lightweight and database-heavy endpoints.

**What are your performance goals?**

**Marcus:** We expect around 500 RPS during peak hours with p95 under 500ms.

**Load Test Assistant:** 

**What load pattern do you want to use?**
- **Ramp-up** (gradual increase) - Good for capacity planning
- **Spike** (sudden burst) - Good for auto-scale testing
- **Sustained** (constant load) - Good for stability testing
- **Step** (incremental) - Good for finding breaking points

**Marcus:** Let's do ramp-up since this is our first load test.

**Load Test Assistant:** 

**How long should the test run and how many virtual users?**

**Marcus:** 5 minutes total, ramping from 50 to 200 virtual users.

**Load Test Assistant:** Perfect. Here's your test configuration:

```yaml
# Load Test Configuration
Target: https://app-saifv2-api-ss4xs2.azurewebsites.net
Pattern: Ramp-up
Duration: 5 minutes

Stages:
  - 0-1 min: Ramp 0 â†’ 50 VUs
  - 1-4 min: Ramp 50 â†’ 200 VUs  
  - 4-5 min: Ramp 200 â†’ 0 VUs (cooldown)

Endpoints:
  - GET / (40% of traffic)
  - GET /api/sqlwhoami (30% of traffic)
  - GET /api/sqlsrcip (30% of traffic)

Thresholds:
  - p95 response time < 500ms
  - Error rate < 1%
  - RPS target: 500
```

**Pre-Test Checklist:**
- [ ] Application Insights monitoring enabled
- [ ] Azure metrics dashboard open
- [ ] No other tests running concurrently
- [ ] Baseline performance documented

Ready to generate the k6 test script?

**Marcus:** Yes, generate the script.

**Load Test Assistant:** Here's your k6 load test script:

```javascript
// load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// Custom metrics
const errorRate = new Rate('errors');
const healthTrend = new Trend('health_duration');
const sqlWhoamiTrend = new Trend('sqlwhoami_duration');
const sqlSrcipTrend = new Trend('sqlsrcip_duration');

// Test configuration
export const options = {
  stages: [
    { duration: '1m', target: 50 },   // Ramp up to 50 VUs
    { duration: '3m', target: 200 },  // Ramp up to 200 VUs
    { duration: '1m', target: 0 },    // Ramp down
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],  // p95 < 500ms
    errors: ['rate<0.01'],              // Error rate < 1%
  },
};

const BASE_URL = __ENV.API_BASE_URL || 'https://app-saifv2-api-ss4xs2.azurewebsites.net';

export default function () {
  // Weighted endpoint distribution
  const rand = Math.random();
  
  if (rand < 0.4) {
    // 40% - Health check (lightweight)
    const res = http.get(`${BASE_URL}/`);
    healthTrend.add(res.timings.duration);
    check(res, {
      'health status 200': (r) => r.status === 200,
      'health response time < 500ms': (r) => r.timings.duration < 500,
    });
    errorRate.add(res.status !== 200);
    
  } else if (rand < 0.7) {
    // 30% - SQL Who Am I (database query)
    const res = http.get(`${BASE_URL}/api/sqlwhoami`);
    sqlWhoamiTrend.add(res.timings.duration);
    check(res, {
      'sqlwhoami status 200': (r) => r.status === 200,
      'sqlwhoami response time < 500ms': (r) => r.timings.duration < 500,
    });
    errorRate.add(res.status !== 200);
    
  } else {
    // 30% - SQL Source IP (database query)
    const res = http.get(`${BASE_URL}/api/sqlsrcip`);
    sqlSrcipTrend.add(res.timings.duration);
    check(res, {
      'sqlsrcip status 200': (r) => r.status === 200,
      'sqlsrcip response time < 500ms': (r) => r.timings.duration < 500,
    });
    errorRate.add(res.status !== 200);
  }
  
  sleep(0.1); // 100ms think time between requests
}
```

Save this to `validation/load-testing/load-test.js` and run with:

```bash
k6 run load-test.js
```

**Marcus:** [runs the test]

```
          /\      |â€¾â€¾| /â€¾â€¾/   /â€¾â€¾/   
     /\  /  \     |  |/  /   /  /    
    /  \/    \    |     (   /   â€¾â€¾\  
   /          \   |  |\  \ |  (â€¾)  | 
  / __________ \  |__| \__\ \_____/ .io

  execution: local
     script: load-test.js
     output: -

  scenarios: (100.00%) 1 scenario, 200 max VUs, 5m30s max duration
           default: Up to 200 looping VUs for 5m0s

running (5m00.0s), 000/200 VUs, 28453 complete iterations
default âœ“ [======================================] 000/200 VUs  5m0s

     âœ“ health status 200
     âœ“ health response time < 500ms
     âœ“ sqlwhoami status 200
     âœ— sqlwhoami response time < 500ms
      â†³  94% â€” âœ“ 8012 / âœ— 510
     âœ“ sqlsrcip status 200
     âœ— sqlsrcip response time < 500ms
      â†³  95% â€” âœ“ 8107 / âœ— 426

     checks.........................: 97.15% âœ“ 53825      âœ— 1576
     data_received..................: 12 MB  41 kB/s
     data_sent......................: 3.2 MB 11 kB/s
     errors.........................: 0.00%  âœ“ 0          âœ— 28453
     health_duration................: avg=145ms  min=89ms   p(95)=245ms  p(99)=312ms
     http_req_duration..............: avg=287ms  min=89ms   p(95)=478ms  p(99)=612ms
   âœ“ http_req_duration..............: p(95)=478ms  < 500ms
     http_reqs......................: 28453  94.84/s
     iteration_duration.............: avg=387ms  min=189ms  p(95)=578ms
     iterations.....................: 28453  94.84/s
     sqlsrcip_duration..............: avg=345ms  min=156ms  p(95)=512ms  p(99)=687ms
     sqlwhoami_duration.............: avg=356ms  min=178ms  p(95)=523ms  p(99)=701ms
     vus............................: 1      min=1        max=200
     vus_max........................: 200    min=200      max=200
```

**Load Test Assistant:** Test complete! Let me analyze the results.

**Performance Summary:**

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| RPS | 500 | 94.84 | âš ï¸ Below target |
| p95 Response Time | < 500ms | 478ms | âœ… Pass |
| p99 Response Time | N/A | 612ms | âš ï¸ Spikes |
| Error Rate | < 1% | 0.00% | âœ… Pass |
| Total Requests | N/A | 28,453 | âœ… |

**Per-Endpoint Analysis:**

| Endpoint | Avg | p95 | p99 | Status |
|----------|-----|-----|-----|--------|
| Health (/) | 145ms | 245ms | 312ms | âœ… Excellent |
| /api/sqlwhoami | 356ms | 523ms | 701ms | âš ï¸ p95 exceeds threshold |
| /api/sqlsrcip | 345ms | 512ms | 687ms | âš ï¸ p95 exceeds threshold |

**Bottleneck Analysis:**

```
ğŸ” IDENTIFIED BOTTLENECK: Database Queries

Evidence:
- Health endpoint (no DB): p95 = 245ms âœ…
- SQL endpoints (with DB): p95 = 512-523ms âš ï¸

Root Cause: Database queries add ~280ms latency under load
Likely factors:
1. Connection pool size may be insufficient
2. Query execution time increases with concurrent connections
3. DTU/vCore capacity on database tier

Recommendation: 
- Check SQL Database DTU usage during test
- Consider increasing connection pool size
- Evaluate database tier upgrade if DTU saturated
```

**Scaling Recommendations:**

| Option | Impact | Cost Impact |
|--------|--------|-------------|
| Increase App Service plan | +30% RPS | +$50/month |
| Upgrade SQL to S2 tier | -40% DB latency | +$75/month |
| Enable connection pooling | -20% response time | $0 |
| Add Redis cache | -50% DB load | +$25/month |

**Performance Grade: B**
- Meets p95 threshold overall
- Database endpoints slightly over threshold
- Error rate excellent
- RPS below target (need investigation)

Should I generate the detailed load test report?

**Marcus:** Yes, and explain why RPS is so low compared to our target.

**Load Test Assistant:** Good question! Let me explain the RPS calculation:

**Why 94 RPS Instead of 500 RPS:**

```
Your Configuration:
- 200 VUs (virtual users)
- Each request takes ~287ms average
- Each VU has 100ms think time between requests

Calculation:
- Time per iteration = 287ms (request) + 100ms (sleep) = 387ms
- Iterations/second per VU = 1000ms / 387ms = 2.58
- Total RPS = 200 VUs Ã— 2.58 = ~516 RPS theoretical

But wait, we only got 94 RPS?

The ramp-up pattern explains it:
- Minutes 0-1: Ramping 0â†’50 VUs (avg ~25 VUs)
- Minutes 1-4: Ramping 50â†’200 VUs (avg ~125 VUs)
- Minutes 4-5: Ramping 200â†’0 VUs (avg ~100 VUs)

Average VUs across test = ~100 VUs (not 200)
Expected RPS = 100 Ã— 2.58 = ~258 RPS

Actual 94 RPS suggests response times were higher than reported average,
likely due to database connection queuing at peak load.
```

**To Actually Achieve 500 RPS:**

Option 1: More VUs (current approach limitations)
- Need ~400 VUs sustained to hit 500 RPS
- But this will increase response times further

Option 2: Reduce response time (better approach)
- Optimize database queries
- Add caching for repeated queries
- With 200ms avg response: 200 VUs Ã— 5 req/s = 1000 RPS

**Report generated at:** `validation/load-testing/load-test-report-[timestamp].md`

---

## Phase 5: Putting It All Together

### The Complete Validation Workflow

**Marcus:** This is really helpful. Can you summarize the complete workflow I should follow
for every deployment?

**Copilot:** Here's your deployment validation checklist:

**Pre-Deployment Validation Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: DEFINE ACCEPTANCE CRITERIA                                      â”‚
â”‚                                                                         â”‚
â”‚   Before any testing, document:                                         â”‚
â”‚   - Functional requirements (what must work)                            â”‚
â”‚   - Performance targets (response times, throughput)                    â”‚
â”‚   - Security requirements (authentication, encryption)                  â”‚
â”‚   - Compliance needs (HIPAA, SOC2, etc.)                               â”‚
â”‚                                                                         â”‚
â”‚   Time: 30 minutes                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: RUN UAT TESTS                                                   â”‚
â”‚                                                                         â”‚
â”‚   @uat-assistant I need to test [service URL]                          â”‚
â”‚                                                                         â”‚
â”‚   - Follow agent prompts                                               â”‚
â”‚   - Run automated test suite                                           â”‚
â”‚   - Review results and generated report                                â”‚
â”‚   - Get sign-off if all tests pass                                     â”‚
â”‚                                                                         â”‚
â”‚   Time: 15-20 minutes                                                  â”‚
â”‚   Gate: Must achieve 100% pass rate to proceed                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: RUN LOAD TESTS                                                  â”‚
â”‚                                                                         â”‚
â”‚   @loadtest-assistant I need to load test [service URL]                â”‚
â”‚                                                                         â”‚
â”‚   - Define performance targets                                         â”‚
â”‚   - Choose appropriate load pattern                                    â”‚
â”‚   - Execute test with monitoring                                       â”‚
â”‚   - Analyze results and bottlenecks                                    â”‚
â”‚   - Get recommendations for optimization                               â”‚
â”‚                                                                         â”‚
â”‚   Time: 20-30 minutes                                                  â”‚
â”‚   Gate: Must achieve Grade B or higher to proceed                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: REVIEW AND SIGN-OFF                                            â”‚
â”‚                                                                         â”‚
â”‚   Review generated reports:                                            â”‚
â”‚   - UAT Report: validation/uat/uat-report-[timestamp].md               â”‚
â”‚   - Load Report: validation/load-testing/load-test-report-[ts].md      â”‚
â”‚                                                                         â”‚
â”‚   Get approvals:                                                       â”‚
â”‚   - QA Engineer: Validates test coverage                               â”‚
â”‚   - Tech Lead: Validates technical decisions                           â”‚
â”‚   - Business Stakeholder: Validates acceptance criteria                â”‚
â”‚                                                                         â”‚
â”‚   Time: 15-20 minutes                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: DEPLOY TO PRODUCTION                                           â”‚
â”‚                                                                         â”‚
â”‚   With all validations complete and signed off:                        â”‚
â”‚   - Deploy using your CI/CD pipeline                                   â”‚
â”‚   - Run smoke tests post-deployment                                    â”‚
â”‚   - Monitor for 24 hours                                               â”‚
â”‚   - Archive test reports for audit trail                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TOTAL TIME: ~1.5 hours (vs. 25+ hours manual)
```

---

### Discovery Questions Summary

**Marcus:** What are the key questions I should ask to really understand testing?

**Copilot:** Here are the discovery questions organized by topic:

**Understanding UAT:**
```
- What makes good acceptance criteria? (SMART pattern)
- How do I know I've tested enough? (Coverage metrics)
- What's the difference between functional and integration tests?
- How do I handle flaky tests? (Retry strategies)
```

**Understanding Load Testing:**
```
- Why p95 instead of average response time?
- What load pattern should I use for my scenario?
- How do I identify bottlenecks from metrics?
- What's the relationship between VUs, RPS, and response time?
```

**Understanding the Agents:**
```
- How does the agent know what to test?
- Why does the agent ask discovery questions?
- How do I customize test plans for my domain?
- What if I disagree with agent recommendations?
```

**Understanding Production Readiness:**
```
- When is UAT enough vs. needing load tests?
- What grade is acceptable for production?
- How do I communicate results to stakeholders?
- What audit trail do I need?
```

---

## Key Takeaways

### For QA Engineers

1. **Agents Guide, You Decide** - The agent structures the workflow; you provide domain expertise
2. **Discovery First** - Good testing starts with good questions (criteria, goals, constraints)
3. **Understand the Metrics** - p95, RPS, error rate tell different stories
4. **Document Everything** - Auto-generated reports create audit trails

### For the Process

1. **UAT Before Load** - Functional correctness first, then performance
2. **Define Targets Upfront** - Know your acceptance criteria before testing
3. **Analyze Bottlenecks** - Don't just pass/fail; understand WHY
4. **Get Sign-offs** - Reports are for stakeholder communication

### Time Investment

| Activity | Manual | With Agents | Savings |
|----------|--------|-------------|---------|
| UAT Planning | 2 hrs | 5 min | 96% |
| UAT Execution | 3 hrs | 15 min | 92% |
| Load Test Setup | 3 hrs | 10 min | 94% |
| Load Test Analysis | 4 hrs | 10 min | 96% |
| Report Generation | 4 hrs | 5 min | 98% |
| **Total** | **16 hrs** | **45 min** | **95%** |

---

## Related Resources

- **UAT Assistant Agent:** `agents/uat-assistant.agent.md`
- **Load Test Assistant Agent:** `agents/loadtest-assistant.agent.md`
- **UAT Test Scripts:** `validation/uat/uat-tests.sh` and `uat-tests.ps1`
- **Report Templates:** `validation/uat/uat-report-template.md`, `validation/load-testing/load-test-report-template.md`
- **Prompt Patterns:** `prompts/effective-prompts.md`

---

*This conversation demonstrates learning service validation patterns through discovery-based
dialogue. The focus is understanding WHY testing approaches work, not just running agents.*

**Character Growth:**
- **Start:** Manual testing taking 25+ hours per deployment
- **End:** Understands UAT vs. load testing, metrics interpretation, agent workflows
- **Key Insight:** AI-assisted testing combines your domain knowledge with testing patterns
